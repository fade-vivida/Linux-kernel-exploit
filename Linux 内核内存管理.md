---
typora-copy-images-to: picture
---

# Linux 内核内存管理

## 1.页框管理

具体内容在之前页框管理中已经讲得很清楚了，这里只具体阐述几个会令人迷惑的地方

### 1.1 PAE

首先我们来看一个问题，在 32 位系统上一般情况下安装 4GB 的内容是没有什么问题的。然而一些大型的服务器需要大于 4GB 的 RAM 来同时运行数以千计的进程，所以我们必须想出一种方式能够扩展 80x86 结构所支持的 RAM 容量。

PAE 机制就是为了解决上述问题而诞生的。PAE 的全称为 "Physical Address Extension"，即物理地址扩展。是为了解决如何在 32 位系统上利用更多的主存而设计的技术。

#### 1.1.1 硬件支持

1. Intel 在它的处理器上把管脚数从 32 增加到 36，这样寻址能力就可以达到 2^36（64GB）
2. 通过设置 cr4 寄存器的物理地址扩展（PAE）标志激活 PAE。

#### 1.1.2 地址转化

但这里就出现了一个问题：如何把 32 位的线性地址转化为 36 位的物理地址呢？

首先我们必须改变原有的分页机制，Intel 为了支持 PAE 对原本的分页机制作了如下调整：

- 64GB 的 RAM 被分成了 2^24 个页框。页表项的物理地址字段也必须扩展：20->24，并且又因为页表项还必须包含 12 个标志位。这样在 PAE 模式下，页表项的总位数就变为：24+12=36位。又因为需要对齐，所以页表项最终大小为 64 位，比不开启 PAE 时的 32 位增加了一倍。这样一个 4KB 的页表包含的表项也发生了变化：1024->512。
- 引入页目录指针表（PDPT），该表只包含 4 个表项（每个表项 64 位）
- cr3 控制寄存器包含一个 27 位的页目录指针表（PDPT）基地址字段。之所以为 27 位，是因为 PDPT 存放在 RAM 的前 4GB，并且在 32 位字节（2^5）的倍数上对齐，因此 27 位足以表示这种表的基地址
- 当把线性地址映射到 4KB 的页时（页目录项中的 PS 标志清0），32位地址按照如下方式进行解释：
  - cr3：指向一个 PDPT
  - 位 31-30：指向 PDPT 中 4 个项中的一个
  - 位 29-21：指向页目录中 512 个项中的一个
  - 位 20-12：指向页表中 512 个项中的一个
  - 位 11-0：4KB 页中的偏移量
- 当把线性地址映射到 2MB 的页时（页目录项中的 PS 标志置1），32位地址按如下方式解释：
  - cr3：指向一个 PDPT
  - 位 31-30：指向 PDPT 中 4 个项中的一个
  - 位 29-21：指向页目录中 512 项中的一个
  - 位 20-0：2MB 页中的偏移



**注：我们其实可以发现，我们只是增加了线性地址的寻址空间，具体方式是通过将相应页目录项或者页表项的物理地址字段增加了位数，来映射更多的 RAM 地址。但同一时间，系统所能寻址的空间大小并没有增加（还是 4GB）。**

### 1.2 PS 标志

关于 PS（Page Size）标志，可以这么理解。当我们需要一块大的连续内存时（大于 4KB），Intel 提供给我们的一种解决方式：

1. 分配的大块内存大小为 4MB（未开启 PAE），或者为 2MB（开启 PAE）
2. 在分页机制中去除第二级（即页表级），在页目录项的标志位中置 PS 标志，然后页目录项的物理地址字段直接指向这块大内存。
3. 这样在没有开启 PAE 的情况下，剩余可以利用的线性地址位数为：32-10=22，可以寻址 4MB 的页。在开启 PAE 时，剩余线性地址位数为：32-2-9=21，可以寻址 2MB 的页。

### 1.3 64 位系统中的分页

我们只关注 x86_64 架构的

线性地址分级为：9+9+9+9+12

也就是说 64 位系统中只有前 48 位有用（符合我们的日常认知，一般情况下 64 位下线性地址为：0xnnmmaabbccdd），按照 4 级模式来解释。



## 2.Linux 分页相关宏和函数定义

**注：Linux 分页机制中的页全局目录表就相当于之前 Intel 结构中的页目录指针表**

### 2.1 线性地址字段

- **PAGE_SHIFT**：Offset 字段的位数，80x86 处理器，该字段为 12。PAGE_SIZE 使用该值返回页的大小（4KB），PAGE_MASK 宏产生值 0xfffff000，用以屏蔽 Offset 字段的所有位

- **PMD_SHIFT**：指定线性地址的 Offset 字段和 Table 字段的总位数；换句话说就是页中间目录项可以映射的区域大小的对数。PMD_SHIFT 宏用来计算由页中间目录的一个单独表项所能映射的区域大小（也就是一个页表所能映射区域的大小）。

  当 PAE 被禁用时，PMD_SHIFT = 12+10（Offset 的 12 位加上来自 Table 的 10 位）。PMD_SIZE = 4MB，PMD_MASK = 0xffc00000。

  当 PAE 被启用时，PMD_SHIFT = 12 + 9（Offset 的 12 位加上来自 Table 的 9 位）。PMD_SIZE = 2MB，PMD_MASK = 0xffe00000。

- **LARGE_PAGE_SIZE**：大型页不使用最后一级页表，所以产生大型页尺寸的 LARGE_PAGE_SIZE 宏等于 PMD_SIZE，而在大型页地址中用于屏蔽 Offset 字段和 Table 字段所有位的 LARGE_PAGE_MASK 宏，就等于 PMD_MASK

- **PUD_SHIFT**：确定页上级目录项能映射的区域大小的对数。通俗点讲 PUD_SIZE 宏用于计算页全局目录中的一个单独页表项所能映射的区域大小。PUD_MASK 宏用于屏蔽 Offset 字段、Table 字段、Middle Dir 字段的所有位。

- **PGDIR_SHIFT**：确定页全局目录项能映射的区域大小的对数。PGDIR_SIZE 宏用于计算页全局目录中一个单独表项所能映射区域的大小。PGDIR_MASK 用法同上。

  注意区分是否开启 PAE 情况下，该值的不同。开启 PAE，PGDIR_SHIFT = 30，不开启 PDGIR_SHIFT = 22。

- **PTRS_PER_PTE，PTRS_PER_PMD，PTRS_PER_PUD 以及 PTRS_PER_PGD**：用于计算页表、页中间目录、页上级目录和页全局目录表中表项的个数。当 PAE 被禁止时，他们产生的值分别为 1024，1，1，1024。当 PAE 被激活时，产生的值分别为 512，512，1 和 4。

其他一些宏定义可具体参考 《深入理解 Linux 内核 第二种 内存寻址》

### 2.2 页分配函数

#### 2.2.1 pgd_alloc(mm)

分配一个新的页全局目录。如果 PAE 被激活，他还分配三个对应用户态线性地址的子页中间目录。参数 mm（内存描述符地址）在 80x86 体系结构上被忽略。

```c++
pgd_t *pgd_alloc(struct mm_struct *mm)
{
	pgd_t *pgd;
	pmd_t *pmds[PREALLOCATED_PMDS];

	pgd = (pgd_t *)__get_free_page(PGALLOC_GFP);
	// 返回一个单独页框的线性地址
	if (pgd == NULL)
		goto out;

	mm->pgd = pgd;

	if (preallocate_pmds(pmds) != 0)
		goto out_free_pgd;

	if (paravirt_pgd_alloc(mm) != 0)
		goto out_free_pmds;

	/*
	 * Make sure that pre-populating the pmds is atomic with
	 * respect to anything walking the pgd_list, so that they
	 * never see a partially populated pgd.
	 */
	spin_lock(&pgd_lock);

	pgd_ctor(mm, pgd);
	pgd_prepopulate_pmd(mm, pgd, pmds);

	spin_unlock(&pgd_lock);

	return pgd;

out_free_pmds:
	free_pmds(pmds);
out_free_pgd:
	free_page((unsigned long)pgd);
out:
	return NULL;
}
```

#### 2.2.2 对页表项操作的宏

- pgd_index(addr)

  ```c++
  #define pgd_index(address) (((address) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
  ```

  找到线性地址 addr 对应的目录项在页全局目录中的索引（相对位置）

- pgd_offset(mm,address)

  接收内存描述符地址 mm 和线性地址 addr 作为参数。这个宏产生地址 addr 在页全局目录中相应表项的线性地址。其中参数 mm 的作用是用来找到页全局目录，mm->pgd 字段保存了指向页全局目录的指针。

  ```c++
  #define pgd_offset(mm, address) ((mm)->pgd + pgd_index((address)))
  ```

  这里的关键理解在于 mm->pgd 保存的是一个 pgd_t 类型的指针。

  ```c++
  struct	mm_struct{
  	......
  	pgd_t * pgd;
  	......
  }
  ```

  <!--也就是说对于指针类型的加法操作，实际加的值为当前系统环境下指针类型的大小，即：32bit，(int*)a + 1 = a + 4；64bit，（int*a) + 1 =a + 8-->

- pgd_offset_k(addr)

  ```c++
  #define pgd_offset_k(address) pgd_offset(&init_mm, (address))
  ```

  产生主内核页全局目录中的某个项的线性地址，该项对应于 addr

- pgd_page(pgd)

  通过页全局目录项 pgd 产生页上级目录所在页框的页描述符地址。在两级或三级分页系统中，该宏等价于 pud_page()，后者应用于页上级目录

- d

- d

- 

  



物理地址与线性地址转化

```c++
#define __pa(kaddr) virt_to_phys((void *)(unsigned long)(kaddr))
#define __va(paddr) phys_to_virt((phys_addr_t)(paddr))

static inline phys_addr_t virt_to_phys(const volatile void *kaddr)
{
	return (phys_addr_t)((unsigned long)kaddr - PAGE_OFFSET);
}

static inline void *phys_to_virt(phys_addr_t paddr)
{
	return (void *)((unsigned long)paddr + PAGE_OFFSET);
}
```

其中 PAGE_OFFSET 的值在 32 位系统下为：0xc0000000



paging_init 函数：

```c++
void __init paging_init(void)
{
	pagetable_init();

	__flush_tlb_all();

	kmap_init();

	/*
	 * NOTE: at this point the bootmem allocator is fully available.
	 */
	olpc_dt_build_devicetree();
	sparse_init();
	zone_sizes_init();
}
```



```c++
static void __init pagetable_init(void)
{
	pgd_t *pgd_base = swapper_pg_dir;

	permanent_kmaps_init(pgd_base);
}

static void __init permanent_kmaps_init(pgd_t *pgd_base)
{
	unsigned long vaddr;
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *pte;

	vaddr = PKMAP_BASE;
	page_table_range_init(vaddr, vaddr + PAGE_SIZE*LAST_PKMAP, pgd_base);

	pgd = swapper_pg_dir + pgd_index(vaddr);
	pud = pud_offset(pgd, vaddr);
	pmd = pmd_offset(pud, vaddr);
	pte = pte_offset_kernel(pmd, vaddr);
	pkmap_page_table = pte;
}
```





```c++
static void __init
page_table_range_init(unsigned long start, unsigned long end, pgd_t *pgd_base)
{
	int pgd_idx, pmd_idx;
	unsigned long vaddr;
	pgd_t *pgd;
	pmd_t *pmd;
	pte_t *pte = NULL;

	vaddr = start;
	pgd_idx = pgd_index(vaddr);
	pmd_idx = pmd_index(vaddr);
	pgd = pgd_base + pgd_idx;

	for ( ; (pgd_idx < PTRS_PER_PGD) && (vaddr != end); pgd++, pgd_idx++) {
		pmd = one_md_table_init(pgd);
		pmd = pmd + pmd_index(vaddr);
		for (; (pmd_idx < PTRS_PER_PMD) && (vaddr != end);
							pmd++, pmd_idx++) {
			pte = page_table_kmap_check(one_page_table_init(pmd),
			                            pmd, vaddr, pte);

			vaddr += PMD_SIZE;
		}
		pmd_idx = 0;
	}
}
```





```c++
static pmd_t * __init one_md_table_init(pgd_t *pgd)
{
	pud_t *pud;
	pmd_t *pmd_table;

#ifdef CONFIG_X86_PAE
	if (!(pgd_val(*pgd) & _PAGE_PRESENT)) 
    {
		if (after_bootmem)
			pmd_table = (pmd_t *)alloc_bootmem_pages(PAGE_SIZE);
		else
			pmd_table = (pmd_t *)alloc_low_page();
		paravirt_alloc_pmd(&init_mm, __pa(pmd_table) >> PAGE_SHIFT);
		set_pgd(pgd, __pgd(__pa(pmd_table) | _PAGE_PRESENT));
		pud = pud_offset(pgd, 0);
		BUG_ON(pmd_table != pmd_offset(pud, 0));

		return pmd_table;
	}
#endif
	pud = pud_offset(pgd, 0);
	pmd_table = pmd_offset(pud, 0);

	return pmd_table;
}
```

该函数传入参数为 pgd_t 类型的一个指针 pgd 。然后执行如下步骤：

- 调用 pgd_val(*pgd) 函数得到当前页全局目录项的值，查看是否设置了 PAGE_PRESENT 标志
  - 如果没有设置 PAGE_PRESENT，表明还没有为该目录项分配空间。此时检查 after_bootmem 标志是否置位。
    - after_bootmem 标志被设置，表示当前使用 bootmem 分配器，则调用 alloc_bootmem_pages() 分配页中间目录
    - 否则，调用 alloc_low_page() 分配
  - 否则，表明相应页全局目录项已有值，则调用











bootmem 分配器

首先我们为什么要使用 bootmem 分配器，内存管理不是有 buddy 系统和 slab 分配器吗？

答案是：在系统初始化的时候需要执行一些内存管理，内存分配的任务，这个时候 buddy 系统，slab 分配器等并没有被初始化好，此时就引入了一种内存管理器 bootmem 分配器在系统初始化的时候进行内存管理与分配，当buddy 系统和 slab 分配器初始化好后，在 mem_init() 中对 bootmem 分配器进行释放，内存管理与分配由buddy 系统，slab 分配器等进行接管。



保留页框池

保留内存的数量（以 KB 为单位）存放在 min_free_kbytes 变量中。

保留池大小取决于 ZONE_DMA 和 ZONE_NORMAL 内存管理区中的页框数目。







一张很关键的图片（描述了内核地址空间（32bit 最后 1GB）的分布）

![](D:\study\linux_kernel\linux kennel memory manager\1.PNG)

从图中我们可以看到，永久内核映射（Persistent kernel mappings）和固定地址映射（Fix-mapped linear addresses）确实是相邻的。中间 VMALLOC_START ~~ VMALLOC_END 为动态映射的内核空间地址范围。

## 3.永久内核映射

如果是通过 alloc_page() 获得了高端内存对应的 page，如何给它找个线性空间？

内核专门为此留出一块线性空间，从 PKMAP_BASE 到 FIXADDR_START ，用于映射高端内存（在 2.6 内核上，这个地址范围是 4G-8M 到 4G-4M 之间）。这个空间就叫做 ”内核永久映射空间” 或者 ”永久内核映射空间”。

这个空间和其它空间使用同样的页目录表，对于内核来说，就是 swapper_pg_dir，对普通进程来说，通过 CR3 寄存器指向。通常情况下，这个空间是 4M 大小，因此仅仅需要一个页表即可（注意理解这句话：一个页表（不是页表项），大小为4K，可以映射4M的空间），内核通过来 pkmap_page_table 寻找这个页表。通过 kmap()，可以把一个 page 映射到这个空间来。

由于这个空间是 4M 大小，最多能同时映射 1024 个 page。因此，对于不使用的的 page，及应该时从这个空间释放掉（也就是解除映射关系），通过 kunmap() ，可以把一个 page 对应的线性地址从这个空间释放出来。

### 3.1 pkmap_page_table && LAST_PKMAP

永久内核映射允许内核建立高端页框到内核地址空间的长期映射。具体方法为使用主内核页表中一个专门的页表（其地址存放在 **pkmap_page_table** 变量中）。页表中的表项数由 **LAST_PKMAP** 宏产生（512 或 1024 项），取决于是否开启 PAE。因此，内核一次最多访问 2MB 或 4MB 的高端内存。

```c++
#ifdef CONFIG_HIGHMEM
static void __init permanent_kmaps_init(pgd_t *pgd_base)
{
	unsigned long vaddr;
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *pte;

	vaddr = PKMAP_BASE;
	page_table_range_init(vaddr, vaddr + PAGE_SIZE*LAST_PKMAP, pgd_base);

	pgd = swapper_pg_dir + pgd_index(vaddr);
	pud = pud_offset(pgd, vaddr);
	pmd = pmd_offset(pud, vaddr);
	pte = pte_offset_kernel(pmd, vaddr);
	pkmap_page_table = pte;
}

```

### 3.2 PKMAP_BASE

该页表映射的线性地址从 **PKMAP_BASE** 开始。下面我们结合源码来看看 PKMAP_BASE 的定义：

```c++
#define FIXADDR_SIZE	(__end_of_permanent_fixed_addresses << PAGE_SHIFT)
#define FIXADDR_BOOT_SIZE	(__end_of_fixed_addresses << PAGE_SHIFT)
#define FIXADDR_START		(FIXADDR_TOP - FIXADDR_SIZE)
#define FIXADDR_BOOT_START	(FIXADDR_TOP - FIXADDR_BOOT_SIZE)

#define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1))	\
		    & PMD_MASK)

```

其中 **__end_of_fixed_addresses** 是固定地址映射枚举类中的最后一个元素（可以认为是固定地址映射的个数），将其左移 **PAGE_SHIFT**，可以理解为固定地址映射所占内存页框的大小。

```c++
#ifdef CONFIG_X86_32
/* used by vmalloc.c, vsyscall.lds.S.
 *
 * Leave one empty page between vmalloc'ed areas and
 * the start of the fixmap.
 */
extern unsigned long __FIXADDR_TOP;
#define FIXADDR_TOP	((unsigned long)__FIXADDR_TOP)

#define FIXADDR_USER_START     __fix_to_virt(FIX_VDSO)
#define FIXADDR_USER_END       __fix_to_virt(FIX_VDSO - 1)
#else
#define FIXADDR_TOP	(VSYSCALL_END-PAGE_SIZE)
```

其中 **VSYSCALL_END** 和 **__FIXADDR_TOP** 定义如下：

```c++
#define VSYSCALL_END (-2UL << 20)	// 0xffffffffffe00000 ?
unsigned long __FIXADDR_TOP = 0xfffff000;
```

也就是说在 32bit 下，FIXADDR_TOP = 0xfffff000。

**固定地址映射的地址空间为**：**FIXADDR_BOOT_START <---> FIXADDR_TOP**，即 0xfffff000 -  (__end_of_fixed_addresses << PAGE\_SHIFT) ~~ 0xffff000。

紧挨着固定地址映射的就是永久内核映射空间，所以 PKMAP_BASE = FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1)。

**永久映射的地址空间为**：**PKMAP_BASE <--> PKMAP_BASE +  PAGE_SIZE * (LAST_PKMAP + 1)**

### 3.3 page_slot && page_address_htable

```c++
static struct page_address_slot *page_slot(struct page *page)
{
	return &page_address_htable[hash_ptr(page, PA_HASH_ORDER)];
}
```

下面我们主要来看看 page_address_htable 到底是什么

```c++
struct page_address_map {
	struct page *page;
	void *virtual;
	struct list_head list;
};

static struct page_address_slot {
	struct list_head lh;			/* List of page_address_maps */
	spinlock_t lock;			/* Protect this bucket's list */
} ____cacheline_aligned_in_smp page_address_htable[1<<PA_HASH_ORDER];
```

一个大小为 128 的结构体数组，该结构体包含两个字段：一个指向 page_address_map 的链表指针头（lh）和一个锁变量（lock）。

这里之所以要用一个 Hash 表是因为：当我们的内存大小远远大于 896M时，如果我们采用通常的数组（或者一个单链表）来保存，那么就需要（内存 - 896M）/4K个页，这样查找起来会相当耗时。因此这里引入了 HASH 算法，采用多个链表，每个 page 通过下文所提到的 HASH 算法，对应到一个链表上（总共有128个链表）。

这里会涉及 linux 内核中两个常用的计算 hash 值的常量，和计算 hash 值的函数（我们只关心32位系统）

```c++
/* 2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1 */
#define GOLDEN_RATIO_PRIME_32 0x9e370001UL

static inline u32 hash_32(u32 val, unsigned int bits)
{
	u32 hash = val * GOLDEN_RATIO_PRIME_32;
	return hash >> (32 - bits);
}
#if BITS_PER_LONG == 32
#define GOLDEN_RATIO_PRIME GOLDEN_RATIO_PRIME_32
#define hash_long(val, bits) hash_32(val, bits)

#define PA_HASH_ORDER	7

static inline unsigned long hash_ptr(void *ptr, unsigned int bits)
{
	return hash_long((unsigned long)ptr, bits);
}
```

总结一下：Hash 算法就是用 page 的指针去乘以黄金分割数（0x9e370001），然后再右移 25 位，即取高 7 位的值作为索引在 page_address_htable 中查找对应项。取到的内容为一个 page_address_slot 结构体，然后在遍历其 lh 字段对应的链表（链表中的元素为 page_address_map 结构体），寻找 page_address_map->page 字段等于 page 的那个元素，则该元素的 virtual 就是对应页框的虚拟地址。

### 3.4 page_address_pool && page_address_init

```c++
static struct list_head page_address_pool;	/* freelist */
static spinlock_t pool_lock;			/* protects page_address_pool */
static struct page_address_map page_address_maps[LAST_PKMAP];
void __init page_address_init(void)
{
	int i;

	INIT_LIST_HEAD(&page_address_pool);
	for (i = 0; i < ARRAY_SIZE(page_address_maps); i++)
		list_add(&page_address_maps[i].list, &page_address_pool);
	for (i = 0; i < ARRAY_SIZE(page_address_htable); i++) {
		INIT_LIST_HEAD(&page_address_htable[i].lh);
		spin_lock_init(&page_address_htable[i].lock);
	}
	spin_lock_init(&pool_lock);
}

```

- 初始化 page_address_maps 数组，该数组总共有 LAST_PKMAP 个元素，元素类型为 page_address_map。将 page_address_maps 数组中的每一项都加入到 page_address_pool 链表头指向的链表中。可以将 page_address_pool 理解为指向保存当前空闲线性地址的一个链表。在系统初始化的时候，肯定默认当前所有永久内核映射的线性地址都还没有被分配。
- 初始化 page_address_htable 的各个链表为空。

### 3.5 page_address

```c++
void *page_address(struct page *page)
{
	unsigned long flags;
	void *ret;
	struct page_address_slot *pas;

	if (!PageHighMem(page))
		return lowmem_page_address(page);

	pas = page_slot(page);
	ret = NULL;
	spin_lock_irqsave(&pas->lock, flags);
	if (!list_empty(&pas->lh)) {
		struct page_address_map *pam;

		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				ret = pam->virtual;
				goto done;
			}
		}
	}
done:
	spin_unlock_irqrestore(&pas->lock, flags);
	return ret;
}
```

该函数接收一个页描述符作为参数，并区分以下两种情况：

1. 如果页框不在高端内存中（PG_highmem 标志为 0），则线性地址总是存在并且是通过计算页框下标，然后将其转换成物理地址，最后根据相应的物理地址得到线性地址。
2. 否则调用 page_slot 函数，传入页描述符指针，得到该页描述符在散列表 page_address_htable 对应的 slot。通过下文我们可以看到 slot 就是一个 page_address_map 的链表
3. 判断该链表是否为空。如果不为空，遍历其每一个元素，如果找到一个元素其 page 字段等于我们要查找的 page，则返回其 virtual 字段，该字段就是当前页框所对应的线性地址。如果遍历完真个链表都没有找到，则返回 NULL。



### 3.6 kmap && kmap_high && map_new_virtual

下面我们来看看，内核是如何分配一个永久内核映射的

```c++
void *kmap(struct page *page)
{
	might_sleep();
	if (!PageHighMem(page))
		return page_address(page);
	return kmap_high(page);
}
EXPORT_SYMBOL(kmap);
```

查看 page 对应的页框是否属于高端内存。如果不是，直接调用 page_address 函数返回其页描述符对应的虚拟地址。否则调用 kmap_high。

```c++
/**
 * kmap_high - map a highmem page into memory
 * @page: &struct page to map
 *
 * Returns the page's virtual memory address.
 *
 * We cannot call this from interrupts, as it may block.
 */
void *kmap_high(struct page *page)
{
	unsigned long vaddr;

	/*
	 * For highmem pages, we can't trust "virtual" until
	 * after we have the lock.
	 */
	lock_kmap();
	vaddr = (unsigned long)page_address(page);
	if (!vaddr)
		vaddr = map_new_virtual(page);
	pkmap_count[PKMAP_NR(vaddr)]++;
	BUG_ON(pkmap_count[PKMAP_NR(vaddr)] < 2);
	unlock_kmap();
	return (void*) vaddr;
}
```

再次调用 page_address 函数查看是否对应页框已被分配（由于中断的原因，可能在 kmap 调用 kmap_high 的过程中，该页框已被别的内核路径分配）。如果该页框还没有被分配，则调用 map_new_virtual 分配虚拟地址，同时修改 pkmap_count 数组对应表项（加 1），表示该虚拟地址已被某个进程（或者内核线程）使用。

```c++
#define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))

static inline unsigned long map_new_virtual(struct page *page)
{
	unsigned long vaddr;
	int count;

start:
	count = LAST_PKMAP;
	/* Find an empty entry */
	for (;;) {
		last_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;
		if (!last_pkmap_nr) {
			flush_all_zero_pkmaps();
			count = LAST_PKMAP;
		}
		if (!pkmap_count[last_pkmap_nr])
			break;	/* Found a usable entry */
		if (--count)
			continue;

		/*
		 * Sleep for somebody else to unmap their entries
		 */
		{
			DECLARE_WAITQUEUE(wait, current);

			__set_current_state(TASK_UNINTERRUPTIBLE);
			add_wait_queue(&pkmap_map_wait, &wait);
			unlock_kmap();
			schedule();
			remove_wait_queue(&pkmap_map_wait, &wait);
			lock_kmap();

			/* Somebody else might have mapped it while we slept */
			if (page_address(page))
				return (unsigned long)page_address(page);

			/* Re-start */
			goto start;
		}
	}
	vaddr = PKMAP_ADDR(last_pkmap_nr);
	set_pte_at(&init_mm, vaddr,
		   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));

	pkmap_count[last_pkmap_nr] = 1;
	set_page_address(page, (void *)vaddr);

	return vaddr;
}

```

该函数总体可以看做一个大的循环，其主要流程如下

- 初始化 count 为 LAST_PKMAP，即总共可以用来进行永久映射的线性地址个数。
- 在 for 循环中，从上一次成功分配的线性地址索引 last_pkmap_nr 开始，遍历 pkmap_count 中的所有计数器。
  - 当找到一个空值时，退出循环，调用 PKAMP_ADDR 宏得到 last_pkmap_nr 对应的线性地址（计算过程就是 PKMAP_BASE + last_pkmap_nr << PAGE_SHIFT）。
  - 然后调用 set_pte_at，设置 pkmap_page_table 中的对应表项（第 last_pkmap_nr 项）
  - 更新 pkmap_count 数组的第 last_pkmap_nr 项为 1，表示该项已被使用
  - 调用 set_page_address 设置 page 结构体中的 virtual 字段为 vaddr，并将该 page 插入到 page_address_htable 散列表中。（关于 set_page_address 函数下面会进行详细分析）
- 由于 last_pkmap_nr 是循环遍历，当 last_pkmap_nr 再次循环到 0 ，表示已经进行完了一轮遍历
  - 会再次从下标为 0 的计数器重新开始搜索（last_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK）
  - 此时 pkmap_page_table 中的相应项可能已经被释放，但由于 TLB 表项没有被刷新而导致不能使用（对应的 pkmap_count 项为 1）。所以当 last_pkmap_nr 再次从 0 开始时，会调用 flush_all_zero_pkmaps 函数把 pkmap_count 为 1 的表项全部清 0
  - 重新设置 count 为 LAST_PKMAP
- 当 count 为 0 时，表示已经没有空闲的 pkmap_page_table 表项可用（即 PKMAP_BASE ~ PKMAP_BASE + LAST_PKMAP << PAGE_SHIFT 的线性地址空间均已被映射到对应的页框）
  - 此时就阻塞当前进程（设置 current->state = TASK_UNINTERRUPTIABLE），并将其加入对应的等待队列（pkmap_map_wait）中，调用 schedule 进行进程调度
  - 当该进程被再次唤醒时，调用 page_address 查看该页框是否在其他进程中已经完成映射，如果还是没有，则从 for 循环重新开始。

### 3.7 set_page_address

```c++
void set_page_address(struct page *page, void *virtual)
{
	unsigned long flags;
	struct page_address_slot *pas;
	struct page_address_map *pam;

	BUG_ON(!PageHighMem(page));

	pas = page_slot(page);
	if (virtual) {		/* Add */
		BUG_ON(list_empty(&page_address_pool));

		spin_lock_irqsave(&pool_lock, flags);
		pam = list_entry(page_address_pool.next,
				struct page_address_map, list);
		list_del(&pam->list);
		spin_unlock_irqrestore(&pool_lock, flags);

		pam->page = page;
		pam->virtual = virtual;

		spin_lock_irqsave(&pas->lock, flags);
		list_add_tail(&pam->list, &pas->lh);
		spin_unlock_irqrestore(&pas->lock, flags);
	} else {		/* Remove */
		spin_lock_irqsave(&pas->lock, flags);
		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				list_del(&pam->list);
				spin_unlock_irqrestore(&pas->lock, flags);
				spin_lock_irqsave(&pool_lock, flags);
				list_add_tail(&pam->list, &page_address_pool);
				spin_unlock_irqrestore(&pool_lock, flags);
				goto done;
			}
		}
		spin_unlock_irqrestore(&pas->lock, flags);
	}
done:
	return;
}
```

- 检查页框是否属于高端内存。
- 调用 page_slot 返回页描述符 page 在 page_address_htable 中所对应的链表 pas。
- 检查传入的 virtual 参数是否为空
  - virtual 不为空，表示为添加操作
    - 判断当前空闲页池是否为空，如果不为空，取链表中的第一个元素 pam（page_address_map 类型），将该元素从 page_address_pool 链表中删除
    - 赋值 pam 的 page 和 virtual 字段为传入的参数 page 和 virtual。并将该 pam 加入到其所对应的 pas 链表中
  - virtual 为空，如果为空表示删除操作
    - 遍历哈希表中对应项所指向的链表 pas 中的每一个元素 pam，如果 pam->page == page，则将该 pam 从链表中删除，并将 pam 加入到 page_address_pool 中，表示该 pam 当前未被使用

## 4.临时内核映射

其实临时内核映射就是分配 **FIXADDR_START ~~ FIXADDR_TOP** 之间的线性地址的某个子集范围（**FIX_KMAP_BEGIN ~~ FIX_KMAP_END**）。

至于我们为什么称 **FIXADDR_START ~~ FIXADDR_TOP** 之间的地址空间为固定映射空间，是由于这个空间的线性地址都用作一个特殊需求。同时在其子集 **FIX_KMAP_BEGIN ~~ FIX_KMAP_END** 之间的地址空间还满足以下两个特点：

1）每个 CPU 占用该范围内的一块空间（大小为：KM_TYPE_NR*4K），互相独立不重叠

2）每个 CPU 占用的那块空间中，又被分为了多个小空间（每个小空间大小是 4K）。且各个小空间的使用目的不相同（具体使用目的定义在 kmap_types.h 中的 km_type 中）

### 4.1 km_type && KM_TYPE_NR

```c++
enum km_type {
KMAP_D(0)	KM_BOUNCE_READ,
KMAP_D(1)	KM_SKB_SUNRPC_DATA,
KMAP_D(2)	KM_SKB_DATA_SOFTIRQ,
KMAP_D(3)	KM_USER0,
KMAP_D(4)	KM_USER1,
KMAP_D(5)	KM_BIO_SRC_IRQ,
KMAP_D(6)	KM_BIO_DST_IRQ,
KMAP_D(7)	KM_PTE0,
KMAP_D(8)	KM_PTE1,
KMAP_D(9)	KM_IRQ0,
KMAP_D(10)	KM_IRQ1,
KMAP_D(11)	KM_SOFTIRQ0,
KMAP_D(12)	KM_SOFTIRQ1,
KMAP_D(13)	KM_SYNC_ICACHE,
KMAP_D(14)	KM_SYNC_DCACHE,
/* UML specific, for copy_*_user - used in do_op_one_page */
KMAP_D(15)	KM_UML_USERCOPY,
KMAP_D(16)	KM_IRQ_PTE,
KMAP_D(17)	KM_NMI,
KMAP_D(18)	KM_NMI_PTE,
KMAP_D(19)	KM_KDB,
/*
 * Remember to update debug_kmap_atomic() when adding new kmap types!
 */
KMAP_D(20)	KM_TYPE_NR
};
```

**km_type** 中定义了不同小空间的使用目的（总共有 **KM_TYPE_NR** 种不同的目的），当要进行一次临时映射的时候，需要指定映射的目的。内核会根据映射目的和当前正在运行的 CPU，找到其所对应的小空间，然后把这个空间的地址作为映射地址返回。这意味着一次临时映射会导致以前的映射被覆盖。

那么对于某个特定的目的（例如 KM_BOUNCE_READ），针对某个特定的 CPU（cpuid 为 0），其所对应的线性地址到底是多少呢？

要回答这个问题，我们需要首先了解下到底什么是固定映射的线性地址以及内核是如果将特定的目的转化为线性地址的。

### 4.2 固定映射的线性地址

首先我们先来了解下什么是固定映射的线性地址（fix-mapped linear address）。这里的固定映射是指对于线性地址而言，存在部分线性地址具有固定的含义（具体含义见枚举宏 **fixed_addresses**）。其对应的物理地址不必等于线性地址减去 0xc0000000，而是可以以任意方式建立。

每个固定映射的线性地址都存放在线性地址第 4 个 GB 的末端（具体位置在 32、64 位系统下计算方式有差异）。

#### 4.2.1 fixed_addresses

代码参考自 linux kernel 2.6.34（**arch/x86/include/asm/fixmap.h**）

```c++
enum fixed_addresses {
#ifdef CONFIG_X86_32
	FIX_HOLE,
	FIX_VDSO,
#else
	VSYSCALL_LAST_PAGE,
	VSYSCALL_FIRST_PAGE = VSYSCALL_LAST_PAGE
			    + ((VSYSCALL_END-VSYSCALL_START) >> PAGE_SHIFT) - 1,
	VSYSCALL_HPET,
#endif
	FIX_DBGP_BASE,
	FIX_EARLYCON_MEM_BASE,
#ifdef CONFIG_PROVIDE_OHCI1394_DMA_INIT
	FIX_OHCI1394_BASE,
#endif
#ifdef CONFIG_X86_LOCAL_APIC
	FIX_APIC_BASE,	/* local (CPU) APIC) -- required for SMP or not */
#endif
#ifdef CONFIG_X86_IO_APIC
	FIX_IO_APIC_BASE_0,
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + MAX_IO_APICS - 1,
#endif
#ifdef CONFIG_X86_VISWS_APIC
	FIX_CO_CPU,	/* Cobalt timer */
	FIX_CO_APIC,	/* Cobalt APIC Redirection Table */
	FIX_LI_PCIA,	/* Lithium PCI Bridge A */
	FIX_LI_PCIB,	/* Lithium PCI Bridge B */
#endif
#ifdef CONFIG_X86_F00F_BUG
	FIX_F00F_IDT,	/* Virtual mapping for IDT */
#endif
#ifdef CONFIG_X86_CYCLONE_TIMER
	FIX_CYCLONE_TIMER, /*cyclone timer register*/
#endif
#ifdef CONFIG_X86_32
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
#ifdef CONFIG_PCI_MMCONFIG
	FIX_PCIE_MCFG,
#endif
#endif
#ifdef CONFIG_PARAVIRT
	FIX_PARAVIRT_BOOTMAP,
#endif
	FIX_TEXT_POKE1,	/* reserve 2 pages for text_poke() */
	FIX_TEXT_POKE0, /* first page is last, because allocation is backward */
#ifdef	CONFIG_X86_MRST
	FIX_LNW_VRTC,
#endif
	__end_of_permanent_fixed_addresses,

	/*
	 * 256 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * If necessary we round it up to the next 256 pages boundary so
	 * that we can have a single pgd entry and a single pte table:
	 */
#define NR_FIX_BTMAPS		64
#define FIX_BTMAPS_SLOTS	4
#define TOTAL_FIX_BTMAPS	(NR_FIX_BTMAPS * FIX_BTMAPS_SLOTS)
	FIX_BTMAP_END =
	 (__end_of_permanent_fixed_addresses ^
	  (__end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS - 1)) &
	 -PTRS_PER_PTE
	 ? __end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS -
	   (__end_of_permanent_fixed_addresses & (TOTAL_FIX_BTMAPS - 1))
	 : __end_of_permanent_fixed_addresses,
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + TOTAL_FIX_BTMAPS - 1,
#ifdef CONFIG_X86_32
	FIX_WP_TEST,
#endif
#ifdef CONFIG_INTEL_TXT
	FIX_TBOOT_BASE,
#endif
	__end_of_fixed_addresses
};
```

该枚举类定义了固定映射的地址类型和地址个数（总共有 __end_of_fixed_addresses 个）

#### 4.2.2 fix_to_virt

该函数用于将一个给定的固定映射的地址类型（idx）转化为其所对应的线性地址。

```c++
unsigned long __FIXADDR_TOP = 0xfffff000;

#define VSYSCALL_END (-2UL << 20)
#define FIXADDR_TOP	(VSYSCALL_END-PAGE_SIZE)
#ifdef CONFIG_X86_32
extern unsigned long __FIXADDR_TOP;
#define FIXADDR_TOP	((unsigned long)__FIXADDR_TOP)
#else
#define FIXADDR_TOP	(VSYSCALL_END-PAGE_SIZE)

#define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
#define __virt_to_fix(x)	((FIXADDR_TOP - ((x)&PAGE_MASK)) >> PAGE_SHIFT)

static __always_inline unsigned long fix_to_virt(const unsigned int idx)
{
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();
	return __fix_to_virt(idx);
}
```

我们先来看下不同系统下 **FIXADDR_TOP** 的计算方法

- 32 位系统下：**FIXADDR_TOP** = __FIXADDR_TOP = 0xfffff000
- 64 位系统下：**FIXADDR_TOP**  = VSYSCALL_END - PAGE_SIZE = 0xffffffffffe00000 - 0x1000 = 0xffffffffffdff000

也就是说在 32 位系统下，固定映射线性地址的末端地址为 0xfffff000。而在 64 位系统下，固定映射的线性地址的末端为 0xffffffffffdff000。

搞清楚 FIXADDR_TOP 后，再理解 **fix_to_virt** 的转化机制就很简单了：

- 判读传入的索引参数（idx）是否小于固定地址的索引范围（__end_of_fixed_addresses）
- 如果确实小于，则转化后的地址为 **FIXADDR_TOP - （idx << PAGE_SHIFT）**

函数 **virt_to_fix()** 执行相反的操作

#### 4.2.3 FIX_KMAP_BEGIN && FIX_KMAP_END

```c++
enum fixed_addresses {
	......
    ......
#ifdef CONFIG_X86_32
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
	......
    ......
    __end_of_fixed_addresses
};
```

我们注意到在 fixed_addresses 枚举类中有两个特殊的枚举量 **FIX_KMAP_BEGIN** 和 **FIX_KMAP_END**。这两个枚举量定义了临时映射的地址范围。从章节 4.1 我们得知，对于每个 CPU 都有 KM_TYPE_NR 个不同类型的临时映射地址，那么也就是说对于某个特定体系结构，总共就有 KM_TYPE_NR*NR_CPUS 个线性地址。因此，假设临时映射的地址从 FIX_KMAP_BEGIN 开始，那么其结束地址 FIX_KMAP_END 的计算公式如下所示：
$$
FIX\_KMAP\_END = FIX\_KMAP\_BEGIN + （KM\_TYPE\_NR*NR\_CPUS）-1
$$

### 4.3 临时内核映射的分配

#### 4.3.1 kmap_atomic

该 API 的原型为：**void *kmap_atomic(struct page *page, enum km_type type)**，有个参数是 type，也就是说具体映射到哪一段是由调用者来决定。如果一个线程调用 kmap_atomic() 去建立映射，它会找到当前 CPU的 slots 中对应自己类型（type）的那个 slot，不管这个 slot 上的映射是否存在，它都会直接占用。

<img src="./picture/kmap_atomic.png" alt="kmap_atomic" style="zoom:80%;" />

这样做的好处是保证了不会等待，而且上一个使用这个 slot 的线程不用主动去 unmap，反正迟早都是要被别的线程占用的，就让这个映射一直保留到被占用为止吧，坏处就是可能一个线程正在使用这个 slot 的映射呢，就被其他线程野蛮的抢去了。

为了解决这个问题，Linux 内核于 2009 年对 kmap_atomic() 采用了另一种实现机制，不再基于 slot，而是类似于stack 的形式。因为 stack 通常都是自顶向下（从内存高位地址向内存低位地址）增长的，与固定地址映射空间的划分方式类似。

#### 4.3.2 kmap_atomic_prot

调用者无非需要的是一个虚拟地址而已，它不管一个 page 具体映射到哪个虚拟地址（就像 kmap 做的那样），原则上说 kmap_atomic 提供的仅仅是底层的一个实现机制，一个接口，它完全可以用不同的方式实现，调用者实在没有必要牵扯进这个底层的细节问题，因此 km_type 是没有必要的，故而 2.6.37 内核中果断地去除了这个km_type，现如今 2.6.39 内核的 kmap_atomic 的实现如下（**kmap_atomic_prot**）：

```c++
void *kmap_atomic_prot(struct page *page, pgprot_t prot)
{
	unsigned long vaddr;
	int idx, type;

	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */
	pagefault_disable();

	if (!PageHighMem(page))
		return page_address(page);

	type = kmap_atomic_idx_push();
	idx = type + KM_TYPE_NR*smp_processor_id();
	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
	BUG_ON(!pte_none(*(kmap_pte-idx)));
	set_pte(kmap_pte-idx, mk_pte(page, prot));

	return (void *)vaddr;
}
```

函数主体流程如下：

- 调用 pagefault_disable 在当前 cpu 上禁止抢占
- 判断是否为高端页框，如果不是直接调用 page_address 返回其地址
- 调用 kmap_atomic_idx_push 函数得到映射类型
- 利用映射类型（type）和当前 CPU 的 id（smp_processor_id() 返回值），计算在固定映射空间中对应的索引值（idx）
- 调用 __fix_to_virt 计算索引值（idx）对应的线性地址
- 设置相应的页表项

kmap_atomic_idx_push 函数

```c++
static inline int kmap_atomic_idx_push(void)
{
	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;

#ifdef CONFIG_DEBUG_HIGHMEM
	WARN_ON_ONCE(in_irq() && !irqs_disabled());
	BUG_ON(idx > KM_TYPE_NR);
#endif
	return idx;
}
```

kmap_atomic() 的时候调用 kmap_atomic_idx_push() 执行 stack push 操作，kmap_atomic_unmap() 的时候调用 kmap_atomic_idx_pop() 执行 stack pop 操作，因此，需要确保 kmap 映射的建立和销毁都是严格嵌套的，像这样：

```c++
vaddr1 = kmap_atomic(page1);
vaddr2 = kmap_atomic(page2);

memcpy(vaddr1, vaddr2, PAGE_SIZE);

kunmap_atomic(vaddr2);
kunmap_atomic(vaddr1);
```









mem_map

在 linux 内核中，所有的物理内存都用 struct page 结构来描述，这些对象以数组形式存放，而这个数组的地址就是 mem_map。内核以节点 node 为单位，每个 node 下的物理内存统一管理，也就是说在表示内存 node 的描述类型 struct pglist_data 中，有 node_mem_map 这个成员，该成员指向当前 node 所拥有的内存页框的页描述符数组。

这里需要注意的一点是 **pglist_data** 和 **pg_data_t** 是一个东西（。。。），它们都是节点的描述符结构体。

```c++
typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;
#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
	struct page *node_mem_map;
#ifdef CONFIG_CGROUP_MEM_RES_CTLR
	struct page_cgroup *node_page_cgroup;
#endif
#endif
#ifndef CONFIG_NO_BOOTMEM
	struct bootmem_data *bdata;
#endif
#ifdef CONFIG_MEMORY_HOTPLUG
	/*
	 * Must be held any time you expect node_start_pfn, node_present_pages
	 * or node_spanned_pages stay constant.  Holding this will also
	 * guarantee that any pfn_valid() stays that way.
	 *
	 * Nests above zone->lock and zone->size_seqlock.
	 */
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
					     range, including holes */
	int node_id;
	wait_queue_head_t kswapd_wait;
	struct task_struct *kswapd;
	int kswapd_max_order;
	enum zone_type classzone_idx;
} pg_data_t;
```

如果系统只有一个 pglist_data 对象，那么此对象下的 node_mem_map 即为全局的页描述符存储数组（全局对象 **mem_map**）。

函数 alloc_node_mem_map() 是针对节点 node 的 node_mem_map 分配函数。

alloc_node_mem_map()

```c++
#ifndef CONFIG_FORCE_MAX_ZONEORDER
#define MAX_ORDER 11
#else
#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
#endif
#define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))

static void __init_refok alloc_node_mem_map(struct pglist_data *pgdat)
{
	/* Skip empty nodes */
    // node_spanned_pages 表示节点的大小，包括洞（以页框为单位）
    // 这里的判断是如果该节点不包含有效的内存页框，则直接返回
	if (!pgdat->node_spanned_pages)
		return;

#ifdef CONFIG_FLAT_NODE_MEM_MAP
	/* ia64 gets its own node_mem_map, before this, without bootmem */
	if (!pgdat->node_mem_map) {
		unsigned long size, start, end;
		struct page *map;

		/*
		 * The zone's endpoints aren't required to be MAX_ORDER
		 * aligned but the node_mem_map endpoints must be in order
		 * for the buddy allocator to function correctly.
		 */
		start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
		end = pgdat->node_start_pfn + pgdat->node_spanned_pages;
		end = ALIGN(end, MAX_ORDER_NR_PAGES);
        // 这里的两个对齐操作很令人迷惑，为什么需要对齐呢？好像有点想法了，具体原因见下文
		size =  (end - start) * sizeof(struct page);
		map = alloc_remap(pgdat->node_id, size);
		if (!map)
			map = alloc_bootmem_node_nopanic(pgdat, size);
        	// 如果 alloc_remap 分配失败，则调用 alloc_bootmem_node_nopanic 进行分配
		pgdat->node_mem_map = map + (pgdat->node_start_pfn - start);
        // 然后更新 node_mem_map，node_mem_map 保存了节点中页描述符的数组
	}
#ifndef CONFIG_NEED_MULTIPLE_NODES
	/*
	 * With no DISCONTIG, the global mem_map is just set as node 0's
	 */
	if (pgdat == NODE_DATA(0)) {
		mem_map = NODE_DATA(0)->node_mem_map;
#ifdef CONFIG_ARCH_POPULATES_NODE_MAP
		if (page_to_pfn(mem_map) != pgdat->node_start_pfn)
			mem_map -= (pgdat->node_start_pfn - ARCH_PFN_OFFSET);
#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
	}
#endif
#endif /* CONFIG_FLAT_NODE_MEM_MAP */
}
```

这里我们先解决一个问题，即为什么要在分配页框描述符前进行对齐？

其实就是一个内核规定，规定了节点（node）所分配的页框数必须按 1024 进行对齐，也就是说 alloc_remap 或者 alloc_bootmem_node_nopanic 函数只能分配 1024 的倍数个页描述符给我们，且起始页描述符地址也必须遵循 1024 进行对齐（也就是说起始地址为 2^15 的倍数，2^10 * 2^5），至于当前节点要用多少那就是节点初始化的事了（如果用不完会形成空洞）。

下面我们举例进行说明：

- 假设当前节点（node0）的 node0->node_start_pfn = 2349，也就是说该节点的起始页框编号为 2349
- 我们按照规定进行对齐，则 start = node0->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1，可以得到 start 为 2048
- 假设当前节点（node0）总共包含 5000 个页框（node0->node_spanned_pages = 5000），则按照对齐规定（end = ALIGN(end, MAX_ORDER_NR_PAGES)），得到 end = 5120
- 计算得到分配给页描述符所需内存大小为 size = 0x18000，计算公式为 size = (end - start)*sizeof(struct page)，这里的 struct page 结构体大小为 32 字节

我们先来看下 alloc_remap 函数

```c++
void *alloc_remap(int nid, unsigned long size)
{
	void *allocation = node_remap_alloc_vaddr[nid];

	size = ALIGN(size, L1_CACHE_BYTES);

	if (!allocation || (allocation + size) >= node_remap_end_vaddr[nid])
		return NULL;

	node_remap_alloc_vaddr[nid] += size;
	memset(allocation, 0, size);

	return allocation;
}
```

**node_remap_alloc_vaddr[nid]** 字段存储了当前节点（nid）所可以分配的内核地址（线性地址），然后会计算该地址加上要分配的 size（会与 L1 Cache 进行对齐，默认配置中 L1_CACHE_BYTES = 32） 后是否会超过其结束地址。如果检查通过，则对该块内存清零后返回。

~~**注：突然发现，之所以叫 alloc_remap 函数貌似和内核随机化有关。对于某个特定系统而言，在不开启内核随机化的情况下，页表被固定分配到了低端内存中（ZONE_NORMAL），那么其线性地址就可以被猜到（加上0xc0000000）。而在开启内核随机化时，使用了一个 kva_start_pfn 变量来表示内核的起始页框（在 init_remap_allocator 函数中会使用该值初始化 node_remap_alloc_vaddr 数组），这样页表的线性地址也就随机化了。**~~

貌似不对，反正就是加了了 kva_start_pfn 的偏移，具体原因后面在分析。

到这其实就已经很清楚了，在 Linux 下（由于使用 UMA 模型），其只包含一个单独的节点。所以说 node_remap_alloc_vaddr[0] 理论上包含了所有的内核地址空间。

我们再来看两个函数 free_area_init_node 和 calculate_node_totalpages。

```c++
void __paginginit free_area_init_node(int nid, unsigned long *zones_size,
		unsigned long node_start_pfn, unsigned long *zholes_size)
{
	pg_data_t *pgdat = NODE_DATA(nid);

	pgdat->node_id = nid;
	pgdat->node_start_pfn = node_start_pfn;
	calculate_node_totalpages(pgdat, zones_size, zholes_size);

	alloc_node_mem_map(pgdat);
#ifdef CONFIG_FLAT_NODE_MEM_MAP
	printk(KERN_DEBUG "free_area_init_node: node %d, pgdat %08lx, node_mem_map %08lx\n",
		nid, (unsigned long)pgdat,
		(unsigned long)pgdat->node_mem_map);
#endif

	free_area_init_core(pgdat, zones_size, zholes_size);
}
```

首先我们来看 free_area_init_node，该函数接收节点号（nid）、管理区大小数组（zones_size）、节点起始页框号（node_start_pfn）、管理区空洞大小数组（zholes_size）所为参数。

- 首先，函数声明一个节点类型的变量 pgdat，并命名其节点号为 nid
- 然后初始化其起始页框为 node_start_pfn
- 调用 calculate_node_totalpages 函数计算当前节点的大小和内存节点大小（不包括空洞），两者都是以页框为单位
- 然后调用 alloc_node_mem_map 函数为表示该节点所拥有页框的页描述符分配空间，也就是初始化节点的 node_mem_map 字段
- 最后，调用 free_area_init_core 函数对节点的管理区结构进行初始化

```c++
static void __meminit calculate_node_totalpages(struct pglist_data *pgdat,
		unsigned long *zones_size, unsigned long *zholes_size)
{
	unsigned long realtotalpages, totalpages = 0;
	enum zone_type i;

	for (i = 0; i < MAX_NR_ZONES; i++)
		totalpages += zone_spanned_pages_in_node(pgdat->node_id, i,
								zones_size);
	pgdat->node_spanned_pages = totalpages;

	realtotalpages = totalpages;
	for (i = 0; i < MAX_NR_ZONES; i++)
		realtotalpages -=
			zone_absent_pages_in_node(pgdat->node_id, i,
								zholes_size);
	pgdat->node_present_pages = realtotalpages;
	printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,
							realtotalpages);
}
```

calculate_node_totalpages 函数会遍历当前节点所拥有的所有管理区（理论上 MAX_NR_ZONES 为 4，多了一个 ZONE_MOVABLE 管理区）。然后计算各个管理区所拥有的页框数量及空洞数量，并用计算结果初始化节点的 node_spanned_pages 和 node_present_pages 字段。

最后，我们再来看看 free_area_init 函数。

由于 Linux 使用 UMA 模型，所以本质上讲在系统初始化时会调用 free_area_init 函数（只对节点 0 进行初始化），可以看到传递给 free_area_init_node 函数的起始页框号为 0，节点管理区数组为 zones_size（理论上讲应该包含了所有页框），空洞数组为空。

```c++
void __init free_area_init(unsigned long *zones_size)
{
	free_area_init_node(0, zones_size,
			__pa(PAGE_OFFSET) >> PAGE_SHIFT, NULL);
}
```



lowmem_page_address()

```c++
static __always_inline void *lowmem_page_address(struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
}
```

其中 __va() 函数前面已经介绍了，就是将一个物理地址转化为线性地址。我们再来看 page_to_pfn 函数

```c++
#define ARCH_PFN_OFFSET		(0UL)

#if defined(CONFIG_FLATMEM)

#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + ARCH_PFN_OFFSET)
```

**注：这里有一点需要注意，Linux 内核支持 3 种内存模型，我们这里只关注平坦内存模型。其余可参见 include/asm-generic/memory_model.h**

有了上面的了解，我们再来理解 page_to_pfn 就很容易了。无非是将 page 与 mem_map（正如上面所说 mem_map 保存所有页表描述符的数组）做减法，得到当前页描述符的序号，然后加上一个 ARCH_PFN_OFFSET（默认为0，即从0号页框开始使用） 后返回。

在 lowmem_page_address 中，然后调用 PFN_PHYS 宏，返回页框号对应的物理地址，在调用 __va 宏返回物理地址对应的线性地址（由于是低端内存，所以采用了直接映射的方式，直接加 0xc0000000）。



下面用一张图来总结 page 与 pfn 的互相查找方法。注意，这里只是 page 与 pfn 的互换，并不涉及物理地址与线性地址的相互转化（由于高端内存的存在，所以物理地址并不总是直接映射为线性地址）。

<img src="D:\study\linux_kernel\picture\page_to_pfn.png" alt="page_to_pfn" style="zoom:80%;" />



## 5.伙伴系统算法

### 5.1 关键数据结构及概念

```c++
struct zone {
	......
    ......
	struct per_cpu_pageset __percpu *pageset;	
	struct free_area	free_area[MAX_ORDER];
	......
    ......
} ____cacheline_internodealigned_in_smp;
```

我们只关心管理区描述符中和伙伴系统相关的几个字段的含义

#### 5.1.1 per_cpu_pageset

```c++

struct per_cpu_pages {
	int count;		/* number of pages in the list */
	int high;		/* high watermark, emptying needed */
	int batch;		/* chunk size for buddy add/remove */

	/* Lists of pages, one per migrate type stored on the pcp-lists */
	struct list_head lists[MIGRATE_PCPTYPES];
};

struct per_cpu_pageset {
	struct per_cpu_pages pcp;
#ifdef CONFIG_NUMA
	s8 expire;
#endif
#ifdef CONFIG_SMP
	s8 stat_threshold;
	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
#endif
};

```

#### 5.1.2 free_area

```c++
#define MIGRATE_UNMOVABLE     0
#define MIGRATE_RECLAIMABLE   1
#define MIGRATE_MOVABLE       2
#define MIGRATE_PCPTYPES      3 /* the number of types on the pcp lists */
#define MIGRATE_RESERVE       3
#define MIGRATE_ISOLATE       4 /* can't allocate from here */
#define MIGRATE_TYPES         5

struct free_area {
	struct list_head	free_list[MIGRATE_TYPES];
	unsigned long		nr_free;
};
```

#### 5.1.3 zonelist

```c++
struct zoneref {
	struct zone *zone;	/* Pointer to actual zone */
	int zone_idx;		/* zone_idx(zoneref->zone) */
};

struct zonelist {
	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
	struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];
#ifdef CONFIG_NUMA
	struct zonelist_cache zlcache;			     // optional ...
#endif
};
```

对于 zonelist 我们需要关注的就是该结构中的 _zonerefs 字段。该字段为一个 zoneref 类型的数组（元素个数为系统中的节点数*每个节点拥有的管理区数，在 Linux 下节点数为1）。

zoneref 结构体包含两个字段：一个指向管理区的指针和一个管理区的索引。

我们再来看看在节点描述符中对于 node_zonelists 字段的定义

```c++
#define MAX_ZONELISTS 1

typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;
	......
} pg_data_t;
```

#### 5.1.3 页面迁移

页面迁移其实是伙伴管理算法中的一部分，鉴于其特殊性，特地另行分析。它是 2007 年的时候，2.6.24 内核版本开发时，新增碎片减少策略（the fragmentation reduction strategy）所引入的。该策略也称之为反碎片技术（anti-gragmentation）。 

根据《深入linux内核架构》的描述，反碎片的由来是因为 Linux 内存管理长期存在一个问题：系统启动并长期运行后，物理内存将会产生很多碎片。暂且假设内存页面数为60，则长期运行后，其页面的使用情况可能将会如下图（灰色为已分配）。

![page migrate 1](./picture\image-20200409091045313.png)

虽然其未被分配的页面仍有25%，但能够申请到的最大页面仅为一页。不过这对用户空间是没有影响的，主要是由于用户态的内存是通过页面映射而得到的。所以不在乎具体的物理页面分布，仍将其映射为连续的一块内存提供给用户态程序使用。于是用户态可以感知的内存如下

![page migrate 2](.\picture\image-20200409091250042.png)

但是对于内核态，碎片则是个严肃的问题，因为大部分物理内存都直接映射到内核的永久映射区里面。如果真的存在碎片，将如第一张图所示，无法映射到比一页更大的内存，这在 2.6.24 之前的版本中是 linux 的短板之一。于是为了解决该问题，则引入了反碎片。

linux内核在内存管理时，将已分配的页面划分为三种类型：

- **不可移动页**：该类型页面在内存中位置固定，不可移动。内核核心大部分内存属于该类型 

- **可回收页**：不能够直接移动，但是可以删除，而内容则可以从某些源重新生成。如文件数据映射的页面则归属此类

- **可移动页**：可以随意移动，分配给用户态程序运行的用户空间页面则为该类。由于是通过页面映射而得，将其复制到新位置后，更新映射表项，重新映射，应用程序是不感知的。 

页面的可迁移性则取决于它属于哪一类。而内核使用的反碎片技术则是基于将具有相同可移动性的页分组的思想来实现的。当出现碎片的情况时，可移动页面将会迁移，将为申请者腾出所需的连续页面空间，由此避免了空闲页面空间过于零碎而无法申请到大块连续内存。也由此，不可移动页面不允许在可移动页面中申请，避免因不可迁移而导致碎片。

其中具体的迁移类型在头文件（include/linux/mmzone.h）进行了定义：

- **MIGRATE_UNMOVABLE**：在内存当中有固定的位置，不能移动。内核的核心分配的内存大多属于这种类型
- **MIGRATE_RECLAIMABLE** ：不能直接移动，但可以删除，其内容页可以从其他地方重新生成，例如，映射自文件的数据属于这种类型，针对这种页，内核有专门的页面回收处理
- **MIGRATE_MOVABLE**：可以随意移动，用户空间应用程序所用到的页属于该类别。它们通过页表来映射，如果他们复制到新的位置，页表项也会相应的更新，应用程序不会注意到任何改变
- **MIGRATE_PCPTYPES**：是per_cpu_pageset，即用来表示每CPU页框高速缓存的数据结构中的链表的迁移类型数目
- **MIGRATE_RESERVE**：保留页，是在前三种的列表中都没用可满足分配的内存块时，就可以从MIGRATE_RESERVE分配
- **MIGRATE_ISOLATE**：用于跨越NUMA节点移动物理内存页，该索引的页不能分配，在大型系统上，它有益于将物理内存页移动到接近于是用该页最频繁地CPU
- **MIGRATE_TYPES**：表示迁移类型的数目。

至于迁移类型的页面管理实际上采用的还是伙伴系统算法的管理方式，内存管理区 zone 的结构里面的 free_area 字段是用于管理各阶内存页面的结构体，而其里面的 free_list 字段则是对各迁移类型进行区分的链表。回顾内存页面释放的函数 __free_pages，其将空闲页面挂回去的时候，是做了迁移类型区分的。也就是意味着页面迁移类型是伴随着伙伴系统算法的内存管理构建，根据迁移类型进行分而治之初始化。

### 5.2 分配块

#### 5.2.1 __rmqueue()

__rmqueue() 函数用来在管理区中找到一个空闲块。该函数需要三个参数：

- zone：管理区描述符指针
- order：请求的空闲页块大小的对数值
- migratetype：迁移类型

```c++
static struct page *__rmqueue(struct zone *zone, unsigned int order,
						int migratetype)
{
	struct page *page;

retry_reserve:
	page = __rmqueue_smallest(zone, order, migratetype);

	if (unlikely(!page) && migratetype != MIGRATE_RESERVE) {
		page = __rmqueue_fallback(zone, order, migratetype);

		/*
		 * Use MIGRATE_RESERVE rather than fail an allocation. goto
		 * is used because __rmqueue_smallest is an inline function
		 * and we want just one call site
		 */
		if (!page) {
			migratetype = MIGRATE_RESERVE;
			goto retry_reserve;
		}
	}

	trace_mm_page_alloc_zone_locked(page, order, migratetype);
	return page;
}
```

- 函数首先调用 \_\_rmqueue\_smallest() 函数从 zone 上分配指定迁移类型的内存页
- 如果 \_\_rmqueue\_smallest() 函数分配失败，并且迁移类型不是 MIGRATE\_RESERVE（保留页）。那么调用 \_\_rmqueue\_fallback() 再次进行分配
- 如果 \_\_rmqueue\_fallback() 函数也分配失败，那么将 migratetype 参数设置为 MIGRATE\_RESERVE 类型，再次调用 \_\_rmqueue\_smallest() 进行分配

#### 5.2.2 __rmqueue_smallest()

```c++
/*
 * Go through the free lists for the given migratetype and remove
 * the smallest available page from the freelists
 */
static inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
						int migratetype)
{
	unsigned int current_order;
	struct free_area * area;
	struct page *page;

	/* Find a page of the appropriate size in the preferred list */
	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
		area = &(zone->free_area[current_order]);
		if (list_empty(&area->free_list[migratetype]))
			continue;

		page = list_entry(area->free_list[migratetype].next,
							struct page, lru);
		list_del(&page->lru);
		rmv_page_order(page);
		area->nr_free--;
		expand(zone, page, order, current_order, area, migratetype);
		return page;
	}

	return NULL;
}
```

再来复习一下管理区描述符 zone 结构体

```c++
#define MIGRATE_UNMOVABLE     0
#define MIGRATE_RECLAIMABLE   1
#define MIGRATE_MOVABLE       2
#define MIGRATE_PCPTYPES      3 /* the number of types on the pcp lists */
#define MIGRATE_RESERVE       3
#define MIGRATE_ISOLATE       4 /* can't allocate from here */
#define MIGRATE_TYPES         5

struct free_area {
	struct list_head	free_list[MIGRATE_TYPES];
	unsigned long		nr_free;
};

struct zone {
	......
    struct free_area	free_area[MAX_ORDER];
	......
} ____cacheline_internodealigned_in_smp;
```

可以看到 zone 的 free_area 字段就是一个 free_area 类型的数组（总共有 MAX_ORDER = 11 个元素），其中每个元素对应一种块大小（2^order）。在 free_arae 结构体中 free_list 字段为一个链表头数组，其每一个元素指向一种迁移类型的空闲链表。

有了上面的先验知识，我们再来理解 __rmqueue_smallest() 函数就简单多了。

- 函数从当前 current_order 开始进行遍历，直到 MAX_ORDER
- 得到 zone 中当前 current_order 指向 free_area 结构（area）
- 判断 area 中 **migraetetype** 类型的空闲页框链表是否为空
- 如果不为空，从该链表中取出一个元素（链入该链表的为页描述符），并将该页描述符从链表中删除
- 调用 rmv_page_order() 函数，设置页描述符的 private 字段为0（因为该页将被使用，所以该字段不再由伙伴系统使用），_mapcount 字段为 -1
- area->nr_free 减 1，表示该大小的块数量减 1
- 调用 expand() 函数开始切分工作。之所以要切分，是因为可能找到的 current_order > order，此时我们需要将多余的页框按照 2^k 次方递减式进行切分，并将其链入对应的 free_list 中

```c++
static inline void expand(struct zone *zone, struct page *page,
	int low, int high, struct free_area *area,
	int migratetype)
{
	unsigned long size = 1 << high;

	while (high > low) {
		area--;
		high--;
		size >>= 1;
		VM_BUG_ON(bad_range(zone, &page[size]));
		list_add(&page[size].lru, &area->free_list[migratetype]);
		area->nr_free++;
		set_page_order(&page[size], high);
	}
}
```

#### 5.2.3 __rmqueue_fallback()

由前面的分析可以知道 \_\_rmqueue\_smallest() 仅是在指定迁移类型的页面中，自底向上按照大小（阶数）遍历查找所需的空闲页面。如果在指定的迁移类型中没有找到，且所需申请的迁移页类型不为 MIGRATE_RESERVE，将会调用 \_\_rmqueue\_fallback() 进行分配。

```c++
/* Remove an element from the buddy allocator from the fallback list */
static inline struct page *
__rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
{
	struct free_area * area;
	int current_order;
	struct page *page;
	int migratetype, i;

	/* Find the largest possible block of pages in the other list */
	for (current_order = MAX_ORDER-1; current_order >= order;
						--current_order) {
		for (i = 0; i < MIGRATE_TYPES - 1; i++) {
			migratetype = fallbacks[start_migratetype][i];

			/* MIGRATE_RESERVE handled later if necessary */
			if (migratetype == MIGRATE_RESERVE)
				continue;

			area = &(zone->free_area[current_order]);
			if (list_empty(&area->free_list[migratetype]))
				continue;

			page = list_entry(area->free_list[migratetype].next,
					struct page, lru);
			area->nr_free--;

			/*
			 * If breaking a large block of pages, move all free
			 * pages to the preferred allocation list. If falling
			 * back for a reclaimable kernel allocation, be more
			 * aggressive about taking ownership of free pages
			 */
			if (unlikely(current_order >= (pageblock_order >> 1)) ||
					start_migratetype == MIGRATE_RECLAIMABLE ||
					page_group_by_mobility_disabled) {
				unsigned long pages;
				pages = move_freepages_block(zone, page,
								start_migratetype);

				/* Claim the whole block if over half of it is free */
				if (pages >= (1 << (pageblock_order-1)) ||
						page_group_by_mobility_disabled)
					set_pageblock_migratetype(page,
								start_migratetype);

				migratetype = start_migratetype;
			}

			/* Remove the page from the freelists */
			list_del(&page->lru);
			rmv_page_order(page);

			/* Take ownership for orders >= pageblock_order */
			if (current_order >= pageblock_order)
				change_pageblock_range(page, current_order,
							start_migratetype);

			expand(zone, page, order, current_order, area, migratetype);
			
			trace_mm_page_alloc_extfrag(page, order, current_order,
				start_migratetype, migratetype);

			return page;
		}
	}

	return NULL;
}
```

可以看到该函数的实现不同于通常的伙伴系统算法（内存页面是由高阶开始递减进行查找），而查找的迁移类型根据 fallbacks 备选类型中进行遍历获得并止于 MIGRATE_RESERVE 类型。由此获得的阶号和迁移类型查找 zone->free_area[]->free_list[] 空闲页面管理链表，如果查找到的话，则将其摘除，否则进入下一类型查找，最后所有类型都查找不到的时候，才会降阶查找。

如果找到了一个空闲区域，且该空闲区域大小的阶数大于等于 5（pageblock_order = 10），或者请求申请的页框类型为 MIGRATE_RECLAIMABLE，那么调用 move_freepages_block() 函数进行页面的移动。

其中 fallbacks 是已确定的类型顺序结构，其定义为：

```c++
/*
 * This array describes the order lists are fallen back to when
 * the free lists for the desirable migrate type are depleted
 */
static int fallbacks[MIGRATE_TYPES][MIGRATE_TYPES-1] = {
	[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,   MIGRATE_RESERVE },
	[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,   MIGRATE_RESERVE },
	[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_RESERVE },
	[MIGRATE_RESERVE]     = { MIGRATE_RESERVE,     MIGRATE_RESERVE,   MIGRATE_RESERVE }, /* Never used */
};
```

下面我们再来看看 move_freepages_block() 函数的实现

#### 5.2.4 move_freepages_block() && move_freepages

这里我们需要注意一点，在新版本的 kernel 源码中，__rmqueue_fallback() 函数不会再直接调用 move_freepages_block()，而是在该函数之上又封装了一层 steal_xxx_fallback() 函数，用来做一些条件检查等操作。

```c++

static int move_freepages_block(struct zone *zone, struct page *page,
				int migratetype)
{
	unsigned long start_pfn, end_pfn;
	struct page *start_page, *end_page;

	start_pfn = page_to_pfn(page);
	start_pfn = start_pfn & ~(pageblock_nr_pages-1);
	start_page = pfn_to_page(start_pfn);
	end_page = start_page + pageblock_nr_pages - 1;
	end_pfn = start_pfn + pageblock_nr_pages - 1;

	/* Do not cross zone boundaries */
	if (start_pfn < zone->zone_start_pfn)
		start_page = page;
	if (end_pfn >= zone->zone_start_pfn + zone->spanned_pages)
		return 0;

	return move_freepages(zone, start_page, end_page, migratetype);
}
```

首先是 move_freepages_block() 函数，该函数为 move_freepages() 函数的上层传参函数。该函数会首先获取传入 page 对应的页框号 start_pfn，然后按照 1024 进行对齐（也就是在物理内存地址上按照 4MB 对齐）。然后将 end_pfn 赋值为 start_pfn + 1023，并判断 start_pfn 和 end_pfn 是否超出当前管理区所拥有的页框范围。如果在范围内，调用 move_freepages() 函数，传入参数为 start_pfn 和 end_pfn 所对应的页描述符 start_page，end_page。

从这里我们可能看不出 move_freepages_block() 对齐操作具体想要干什么，我们接下来看看 move_freepages() 函数的实现

```c++
static int move_freepages(struct zone *zone,
			  struct page *start_page, struct page *end_page,
			  int migratetype)
{
	struct page *page;
	unsigned long order;
	int pages_moved = 0;

#ifndef CONFIG_HOLES_IN_ZONE
	/*
	 * page_zone is not safe to call in this context when
	 * CONFIG_HOLES_IN_ZONE is set. This bug check is probably redundant
	 * anyway as we check zone boundaries in move_freepages_block().
	 * Remove at a later date when no bug reports exist related to
	 * grouping pages by mobility
	 */
	BUG_ON(page_zone(start_page) != page_zone(end_page));
#endif

	for (page = start_page; page <= end_page;) {
		/* Make sure we are not inadvertently changing nodes */
		VM_BUG_ON(page_to_nid(page) != zone_to_nid(zone));

		if (!pfn_valid_within(page_to_pfn(page))) {
			page++;
			continue;
		}

		if (!PageBuddy(page)) {
			page++;
			continue;
		}

		order = page_order(page);
		list_move(&page->lru,
			  &zone->free_area[order].free_list[migratetype]);
		page += 1 << order;
		pages_moved += 1 << order;
	}

	return pages_moved;
}
```

该函数从 start_page 到 end_page 开始遍历每个页描述符。如果当前页描述符对应的页框为空闲状态（PageBuddy 判断 page->_mapcount 是否为 PAGE_BUDDY_MAPCOUNT_VALUE），那么调用 page_order() 函数获取该空闲区域的大小对应的阶数，并将该空闲区域移动到 **migratetype** 参数所对应的 free_list 链表中。然后 page 后移到该空闲区域的末端，并将 pages_moved 加上该块空闲区域的大小（统计总共移动了多大的空闲块），再次循环。

#### 5.2.5 一个分配实例

什么意思呢？我们结合上面的函数举个例子进行分析：

假设我们当前需要分配 256 个连续的页框（对应大小的阶数为 8），页面迁移类型为 MIGRATE_MOVABLE，那么内核会按照如下流程进行分配

1. 首先会调用 __rmqueue_smallest() 函数在 zone->free_area[8~10]->free_list[MIGRATE_MOVABLE] 中进行查找，如果正好找到一个空闲块，那么皆大欢喜，直接返回。

2. 如果在所有 zone->free_area[8~10]->free_list[MIGRATE_MOVEABLE] 中都找不到空闲的块，也就是说当前系统中能满足请求大小的 MIGRATE_MOVEABLE 类型的页框都被用尽了。将调用 __rmqueue_fallback() 从高阶内存开始（递减），遍历其迁移类型的所有备选类型（fallbacks）进行分配。

   针对本次实例，MIGRATE_MOVEABLE 类型的备选类型为：

   MIGRATE_RECLAIMABLE，MIGRATE_UNMOVABLE，MIGRATE_RESERVE

   我们假设在阶数为 9 的迁移页类型为 MIGRATE_RECLAIMABLE 空闲链表（zone->free_area[9]->free_list[MIGRATE_RECLAIMABLE]）中发现了一个空闲区域。

3. 当前分配到的空闲区域大小的阶数为 9（大于 pageblock_order >> 1），调用 move_freepages_block() 函数。

   首先按照 block 进行对齐。这里内核中的 block 其实就是指包含了 1024 个页框的区域。假设我们分配到的空闲页框的起始页框号（start_pfn）为 3452。按照 block 对齐后，start_pfn = 3072，end_pfn = 4095。判断是否超出当前管理区拥有的页框范围，如果在范围内，调用 move_freepages() 函数（传入参数为 start_page 和 end_page，由 start_pfn 和 end_pfn 转化而来）。

4. 该函数从 start_page 开始进行遍历，直到 end_page 为止。如果当前 page 所对应区域为空（page->_mapcount = PAGE_BUDDY_MAPCOUNT_VALUE），获取该区域的大小（page->private 字段），并将该区域从 MIGRATE_RECLAIMABLE 类型链表中剔除，加入到 MIGRATE_MOVABLE 类型对应链表中。然后 page 后移到当前空闲区域末端，继续遍历。如果碰到正在使用的页框直接跳过。最后返回移动页框的总大小 pages_moved。

   **注：这里有一点需要注意，理论上我们在其他迁移类型的链表中找到合适的空闲块了直接返回就好了（顶多再改下类型，由当前类型改为申请迁移页的类型），为什么还要从该空闲块起始页框开始遍历 1024 个页框，做页迁移呢？**

   其实这里可以认为在一定程度上是为了满足局部性原理，加快分配。我们可以想象，当我们需要分配一个迁移类型的空间时，有很大概率在之后会再次进行分配相同类型的页。如果我们在第一次直接返回，那么在下面的分配时还需要再次遍历。所以不如在本次遍历时，就多迁移一些空闲空间过去，下次就不用再次遍历了。

5. 当我们完成页面移动后，再将 page 所对应的块从 MIGRATE_MOVABLE 链表上拆下来。在进行切分并将剩余部分加入空闲链表。

### 5.3 Buddy heart（伙伴系统核心算法）

#### 5.3.1 gfp_zone()

注：之所以所有的分配标志都以 gfp 开头，其实 gfp 就是 get_free_page 的缩写。

该函数以分配标志作为输入，返回该标志对应的管理区类型

```c++
#define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)
static inline enum zone_type gfp_zone(gfp_t flags)
{
	enum zone_type z;
	int bit = (__force int) (flags & GFP_ZONEMASK);

	z = (GFP_ZONE_TABLE >> (bit * ZONES_SHIFT)) &
					 ((1 << ZONES_SHIFT) - 1);

	if (__builtin_constant_p(bit))
		BUILD_BUG_ON((GFP_ZONE_BAD >> bit) & 1);
	else {
#ifdef CONFIG_DEBUG_VM
		BUG_ON((GFP_ZONE_BAD >> bit) & 1);
#endif
	}
	return z;
}
```



#### 5.3.1 alloc_pages()

该函数会继续调用一些传参函数 alloc\_pages\_node()，\_\_alloc\_pages() 等，最终功能实现函数为：**__alloc_pages_nodemask()**

```c++
/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
			struct zonelist *zonelist, nodemask_t *nodemask)
{
	enum zone_type high_zoneidx = gfp_zone(gfp_mask);
	struct zone *preferred_zone;
	struct page *page;
	int migratetype = allocflags_to_migratetype(gfp_mask);

	gfp_mask &= gfp_allowed_mask;

	lockdep_trace_alloc(gfp_mask);

	might_sleep_if(gfp_mask & __GFP_WAIT);

	if (should_fail_alloc_page(gfp_mask, order))
		return NULL;

	/*
	 * Check the zones suitable for the gfp_mask contain at least one
	 * valid zone. It's possible to have an empty zonelist as a result
	 * of GFP_THISNODE and a memoryless node
	 */
	if (unlikely(!zonelist->_zonerefs->zone))
		return NULL;

	get_mems_allowed();
	/* The preferred zone is used for statistics later */
	first_zones_zonelist(zonelist, high_zoneidx,
				nodemask ? : &cpuset_current_mems_allowed,
				&preferred_zone);
	if (!preferred_zone) {
		put_mems_allowed();
		return NULL;
	}

	/* First allocation attempt */
	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
			zonelist, high_zoneidx, ALLOC_WMARK_LOW|ALLOC_CPUSET,
			preferred_zone, migratetype);
	if (unlikely(!page))
		page = __alloc_pages_slowpath(gfp_mask, order,
				zonelist, high_zoneidx, nodemask,
				preferred_zone, migratetype);
	put_mems_allowed();

	trace_mm_page_alloc(page, order, gfp_mask, migratetype);
	return page;
}
EXPORT_SYMBOL(__alloc_pages_nodemask);
```

其中比较关键的函数为：first_zones_zonelist()，get_page_from_freelist()，__alloc_pages_slowpath()。下面我们会对这些函数一一进行分析。

**first_zones_zonelist()**

```c++
static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
					enum zone_type highest_zoneidx,
					nodemask_t *nodes,
					struct zone **zone)
{
	return next_zones_zonelist(zonelist->_zonerefs, highest_zoneidx, nodes,
								zone);
}
/* Returns the next zone at or below highest_zoneidx in a zonelist */
struct zoneref *next_zones_zonelist(struct zoneref *z,
					enum zone_type highest_zoneidx,
					nodemask_t *nodes,
					struct zone **zone)
{
	/*
	 * Find the next suitable zone to use for the allocation.
	 * Only filter based on nodemask if it's set
	 */
	if (likely(nodes == NULL))
		while (zonelist_zone_idx(z) > highest_zoneidx)
			z++;
	else
		while (zonelist_zone_idx(z) > highest_zoneidx ||
				(z->zone && !zref_in_nodemask(z, nodes)))
			z++;

	*zone = zonelist_zone(z);
	return z;
}
```

first_zones_zonelist() 只是对 next_zones_zonelist() 的一个简单封装。

